MASTER01=172.31.37.212
MASTER02=172.31.37.42
WORKER01=172.31.43.255

if [ -f ~/.ssh/id_rsa ];
then echo '' > /dev/null;
else ssh-keygen -q -t rsa -N '' -f ~/.ssh/id_rsa; fi

echo "$(sudo cat ~/.ssh/id_rsa.pub)" | sudo tee -a ~/.ssh/authorized_keys > /dev/null
echo "$(sudo cat ~/.ssh/id_rsa.pub)" | xargs -I % echo 'echo "%" | sudo tee -a ~/.ssh/authorized_keys'

# Install java for hadoop
sudo yum install -y java-1.8.0-openjdk-devel

# Install hadoop
curl -L https://dlcdn.apache.org/hadoop/common/hadoop-3.3.1/hadoop-3.3.1.tar.gz | sudo tar -xz
sudo mv hadoop-3.3.1 /usr/local
sudo ln -s /usr/local/hadoop-3.3.1 /usr/local/hadoop

# Install zookeeper
curl -L https://dlcdn.apache.org/zookeeper/zookeeper-3.7.1/apache-zookeeper-3.7.1-bin.tar.gz | sudo tar -xz
sudo mv apache-zookeeper-3.7.1-bin /usr/local
sudo ln -s /usr/local/apache-zookeeper-3.7.1-bin /usr/local/zookeeper

# Set variables
echo "export JAVA_HOME=$(readlink -f $(which java))" | sed 's%/jre/bin/java%%' | sudo tee -a ~/.bashrc
echo "export HADOOP_HOME=/usr/local/hadoop" | sudo tee -a ~/.bashrc
echo "export HADOOP_CONFIG_HOME=\$HADOOP_HOME/etc/hadoop" | sudo tee -a ~/.bashrc
echo "export ZOOKEEPER_HOME=/usr/local/zookeeper" | sudo tee -a ~/.bashrc
echo "export PATH=\$PATH:\$JAVA_HOME/bin:\$HADOOP_HOME/bin:\$HADOOP_HOME/sbin" | sudo tee -a ~/.bashrc
source ~/.bashrc

# Set hosts
cat <<EOF | sudo tee -a /etc/hosts
$MASTER01 MASTER01 NN1 JN1 CLUSTER
$MASTER02 MASTER02 NN2 DN1 JN2
$WORKER01 WORKER01 DN2 JN3
EOF

# Set hdfs-site.xml
# as many as you want but match with the config
sudo mkdir -p /hdfs_data/hdfs01/
sudo mkdir -p /hdfs_data/hdfs02/
sudo chown -R $(id -u):$(id -g) /hdfs_data
cat <<EOF | tee $HADOOP_HOME/etc/hadoop/hdfs-site.xml
<configuration>
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>
            /hdfs_data/hdfs01/namenode,
            /hdfs_data/hdfs02/namenode
        </value>
    </property>
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>
            /hdfs_data/hdfs01/datanode,
            /hdfs_data/hdfs02/datanode
        </value>
    </property>
    <property>
        <name>dfs.journalnode.edits.dir</name>
        <value>/hdfs_data/hdfs01/journalnode</value>
    </property>

    <property>
        <name>dfs.nameservices</name>
        <value>CLUSTER</value>
    </property>
    <property>
        <name>dfs.ha.namenodes.CLUSTER</name>
        <value>
            NN1,
            NN2
        </value>
    </property>
    <property>
        <name>dfs.namenode.rpc-address.CLUSTER.NN1</name>
        <value>NN1:8020</value>
    </property>
    <property>
        <name>dfs.namenode.http-address.CLUSTER.NN1</name>
        <value>NN1:50070</value>
    </property>
    <property>
        <name>dfs.namenode.rpc-address.CLUSTER.NN2</name>
        <value>NN2:8020</value>
    </property>
    <property>
        <name>dfs.namenode.http-address.CLUSTER.NN2</name>
        <value>NN2:50070</value>
    </property>
    <property>
        <name>dfs.namenode.shared.edits.dir</name>
        <value>qjournal://JN1:8485;JN2:8485;JN3:8485/CLUSTER</value>
    </property>
    <property>
        <name>dfs.client.failover.proxy.provider.CLUSTER</name>
        <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
    </property>
    <property>
        <name>dfs.ha.fencing.methods</name>
        <value>sshfence</value>
    </property>
    <property>
        <name>dfs.ha.fencing.ssh.private-key-files</name>
        <value>$HOME/.ssh/id_rsa</value>
    </property>
    <property>
        <name>dfs.ha.automatic-failover.enabled</name>
        <value>true</value>
    </property>
    <property>
        <name>dfs.webhdfs.enabled</name>
        <value>true</value>
    </property>
    <property>
        <name>dfs.permissions.enabled</name>
        <value>false</value>
    </property>
    <!--
    <property>
        <name>dfs.hosts</name>
        <value>$HADOOP_HOME/etc/hadoop/workers</value>
    </property>
    -->
</configuration>
EOF

cat <<EOF | tee $HADOOP_HOME/etc/hadoop/core-site.xml
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://CLUSTER</value>
    </property>
    <property>
        <name>ha.zookeeper.quorum</name>
        <value>
            JN1:32181,
            JN2:32181,
            JN3:32181
        </value>
    </property>
</configuration>
EOF

cat <<EOF | tee $HADOOP_HOME/etc/hadoop/yarn-site.xml
<configuration>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <property>
        <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
        <value>org.apache.hadoop.mapred.ShuffleHandler</value>
    </property>
    <property>
        <name>yarn.nodemanager.local-dirs</name>
        <value>
            /hdfs_data/hdfs01/yarn/nm-local-dir,
            /hdfs_data/hdfs02/yarn/nm-local-dir
        </value>
    </property>
    <property>
        <name>yarn.resourcemanager.fs.state-store.uri</name>
        <value>
            /hdfs_data/hdfs01/yarn/system/rmstore,
            /hdfs_data/hdfs02/yarn/system/rmstore
        </value>
    </property>

    <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>MASTER01</value>
    </property>
    <property>
        <name>yarn.web-proxy.address</name>
        <value>0.0.0.0:8089</value>
    </property>
</configuration>
EOF

# cat <<EOF | tee $HADOOP_HOME/etc/hadoop/hadoop-env.sh
# export HADOOP_CONF_DIR=${HADOOP_HOME}/etc/hadoop
# # export HADOOP_HEAPSIZE_MAX=16G
# # export HADOOP_HEAPSIZE_MIN=16G
# # export HADOOP_OPTS="-XX:+UseG1GC"
# export HADOOP_OS_TYPE=${HADOOP_OS_TYPE:-$(uname -s)}
# export HADOOP_WORKERS="${HADOOP_CONF_DIR}/workers"
# export HADOOP_LOG_DIR=${HADOOP_HOME}/logs
# # export HDFS_DATANODE_OPTS="-Xms32768m -Xmx32768m"
# export JAVA_HOME=${JAVA_HOME}
# export HDFS_NAMENODE_USER="centos"
# export HDFS_DATANODE_USER="centos"
# export HDFS_SECONDARYNAMENODE_USER="centos"
# export YARN_RESOURCEMANAGER_USER="centos"
# export YARN_NODEMANAGER_USER="centos"
# # hadoop ha
# export HADOOP_PID_DIR=${HADOOP_HOME}/pids
# export HADOOP_SECURE_PID_DIR=${HADOOP_PID_DIR}
# EOF

# cat <<EOF | tee -a $HADOOP_HOME/etc/hadoop/workers
# WORKER_01
# WORKER_02
# EOF

# config zookeeper
cp $ZOOKEEPER_HOME/conf/zoo_sample.cfg $ZOOKEEPER_HOME/conf/zoo.cfg
sudo sed -i "s%dataDir=/tmp/zookeeper%dataDir=$ZOOKEEPER_HOME/data%" $ZOOKEEPER_HOME/conf/zoo.cfg
cat <<EOF | tee -a $ZOOKEEPER_HOME/conf/zoo.cfg
dataLogDir=$ZOOKEEPER_HOME/logs
maxClientCnxns=0
maxSessionTimeout=180000
server.1=MASTER01:2888:3888
server.2=MASTER02:2888:3888
server.3=WORKER01:2888:3888
EOF
sudo mkdir -p $ZOOKEEPER_HOME/data
sudo mkdir -p $ZOOKEEPER_HOME/logs
######## myid should match with zoo.cfg ########
echo 1 | sudo tee $ZOOKEEPER_HOME/data/myid
sudo chown -R $(id -u):$(id -g) $ZOOKEEPER_HOME/data
sudo chown -R $(id -u):$(id -g) $ZOOKEEPER_HOME/logs

# Start zookeeper server
$ZOOKEEPER_HOME/bin/zkServer.sh start

## format전 해주어야하는 작업
cd /usr/local/hadoop/sbin
stop-all.sh
start-all.sh

# Init format (namenode 1)
hdfs namenode -format

####################################
######## SPECIFIC OPERATION ########
####################################
# At journal nodes
hdfs --daemon start journalnode

# At namenode 1
hdfs --daemon start namenode

# At namenode but not 1
hdfs --daemon start namenode
hdfs namenode -bootstrapStandby
# hdfs --daemon start namenode

# At every namenode
hdfs --daemon start zkfc

# At every datanode
hdfs --daemon start datanode

# At namenode 1
start-yarn.sh

# At every namenode
mapred --daemon start historyserver

# namenode 1
start-dfs.sh

#master01 stanby 상태 -> active 상태로
hdfs haadmin -transitionToActive NN1 --forcemanual

# 데이터 저장할 폴더 생성 /data/weather/
hdfs dfs -mkdir /data
hdfs dfs -mkdir /data/weather